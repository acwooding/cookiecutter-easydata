{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cookiecutter Easydata A python framework and git template for data scientists, teams, and workshop organizers aimed at making your data science work reproducible and shareable For most of us, data science is 5% science, 60% data cleaning, and 35% IT hell. Easydata focuses the 95% by helping you deliver reproducible python environments, reproducible datasets, and reproducible workflows In other words, Easydata is a template, library, and workflow that lets you get up and running with your data science analysis, quickly and reproducibly . What is Easydata? Easydata is a python cookiecutter for building custom data science git repos that provides: An opinionated workflow for collaboration, storytelling, A python framework to support this workflow, A makefile wrapper for conda and pip environment management, A catalog of prebuilt dataset recipes , and A library of training materials and documentation around doing reproducible data science. Easydata is not an ETL tooklit A data analysis pipeline a containerization solution, or a prescribed data format. Origins EasyData started life as an opinionated fork of the cookiecutter-datascience project. Easydata has evolved considerably since then with a specific focus on enabling overall team efficiency by improving collaboration and reproducibility. We owe that project a great debt for the work they have done in creating a flexible but highly useful project template. Contributing The EasyData project is opinionated, but not afraid to be wrong. Best practices change, tools evolve, and lessons are learned. The goal of this project is to make it easier to start, structure, and share your data science work. Pull requests and filing issues is encouraged. We'd love to hear what works for you, and what doesn't. If you use the Cookiecutter Easydata Project, link back to this page. Links to related projects and references This project started as a fork of the cookiecutter-datascience project, so we owe them a huge debt. Also, a huge thanks to the Cookiecutter project ( github ), which is helping us all spend less time thinking about and writing boilerplate and more time getting things done.","title":"Home"},{"location":"#cookiecutter-easydata","text":"A python framework and git template for data scientists, teams, and workshop organizers aimed at making your data science work reproducible and shareable For most of us, data science is 5% science, 60% data cleaning, and 35% IT hell. Easydata focuses the 95% by helping you deliver reproducible python environments, reproducible datasets, and reproducible workflows In other words, Easydata is a template, library, and workflow that lets you get up and running with your data science analysis, quickly and reproducibly .","title":"Cookiecutter Easydata"},{"location":"#what-is-easydata","text":"Easydata is a python cookiecutter for building custom data science git repos that provides: An opinionated workflow for collaboration, storytelling, A python framework to support this workflow, A makefile wrapper for conda and pip environment management, A catalog of prebuilt dataset recipes , and A library of training materials and documentation around doing reproducible data science. Easydata is not an ETL tooklit A data analysis pipeline a containerization solution, or a prescribed data format.","title":"What is Easydata?"},{"location":"#origins","text":"EasyData started life as an opinionated fork of the cookiecutter-datascience project. Easydata has evolved considerably since then with a specific focus on enabling overall team efficiency by improving collaboration and reproducibility. We owe that project a great debt for the work they have done in creating a flexible but highly useful project template.","title":"Origins"},{"location":"#contributing","text":"The EasyData project is opinionated, but not afraid to be wrong. Best practices change, tools evolve, and lessons are learned. The goal of this project is to make it easier to start, structure, and share your data science work. Pull requests and filing issues is encouraged. We'd love to hear what works for you, and what doesn't. If you use the Cookiecutter Easydata Project, link back to this page.","title":"Contributing"},{"location":"#links-to-related-projects-and-references","text":"This project started as a fork of the cookiecutter-datascience project, so we owe them a huge debt. Also, a huge thanks to the Cookiecutter project ( github ), which is helping us all spend less time thinking about and writing boilerplate and more time getting things done.","title":"Links to related projects and references"},{"location":"datasets/","text":"Datasets Easydata lets you build a dataset catalog from which you can load any dataset in the catalog via its dataset_name via the .load API. ds = Dataset.load(dataset_name) The basic idea is that we don't want to share data directly, instead, we share the recipes for how to re-create Datasets. These recipes are stored in the dataset catalog. Datasets can then be shared by sharing the catalog. Datasets are the fundamental object that makes sharing of datasets reproducible, as they keep track of their own recipes, check that the data created from a recipe has the correct hashes, and keep licenses and other metadata with the data itself. What is a Dataset object? A Dataset is the fundamental object we use for turning raw data into useful datasets, reproducibly. It is like a scikit-learn-style Bunch object---essentially, a dictionary with some extra magic to make it nicer to work with---containing the following attributes: data: the processed data target: (optional) target vector (for supervised learning problems) metadata: Data about the data The data attribute can really be any processed data form that you like: sometimes it's a pandas dataframe, a list of tuples containing other data, or other formats including scipy.sparse matrices or igraph graphs. The target (if you're using it), expects something that matches the data in terms of length. For a hint as to which data format to expect, you can look at the contents of the DESCR attribute, one of the many pieces of medata that are maintained as part of the Dataset object. This metadata is where things get interesting... which we'll cover on its own next. Why metadata ? The metadata is where the magic lives. It serves several purposes in terms of bookkeeping: it includes HASHES , which improve data reproducibility , since what you download and process gets checked each step along the way to ensure the raw data matches what is stored in the dataset_catalog , it provides easy access to what the data is via the DESCR attribute, it provides easy (and continual) access to the license / usage restrictions for the data (the LICENSE attribute), which helps with knowing what you can do when Sharing your Work . it provides the extra data manifest , EXTRA , if your dataset includes around additional raw data (extra) files. In short, it helps you to know what data you're working with, what you can do with it, and whether something has gone wrong. Under the hood, metadata is a dictionary; however metadata can also be accessed by referring to attributes expressed in uppercase. For example, ds.metadata['license'] and ds.LICENSE refer to the same thing. Using a Dataset As mentioned before, to load a Dataset : ds = Dataset.load(\"<dataset-name>\") At this point, if you already have a cached copy of the desired Dataset on disk, it will load it. Otherwise, the it will follow the recipe for generating the requested Dataset ; i.e. generate the dataset from raw data, as per the instructions contained in the dataset_catalog (described below). Because of licenses and other distribution restrictions, some of the datasets may require a manual download step. If so, you will be prompted at this point and given instructions for what to do. Some datasets will require local pre-processing. If so, the first time your run the command, you will be executing all of the processing scripts (which can be quite slow). After the first load, however, datasets will load from cache on disk which should be fast. If you need to free up space, you can even delete related source files from data/raw and data/interim . Just don't touch the data/processed directory. To access the data, target or metdata: ds.data ds.target ds.metadata To access the most common metadata fields: ds.DESCR # or ds.metadata['descr'] ds.LICENSE # or ds.metadata['license'] ds.HASHES # or ds.metadata['hashes'] The Dataset catalog You can explore all of the currently available Datasets via the dataset catalog. The catalog keeps a record of the recipes used to generate a Dataset along with relevant hashes that are used to ensure the integrity of data when it's loaded. To access the catalog: workflow.available_datasets(keys_only=True) If you're interested, set keys_only=False to see the complete contents of the metadata that is saved in the catalog.","title":"Datasets"},{"location":"datasets/#datasets","text":"Easydata lets you build a dataset catalog from which you can load any dataset in the catalog via its dataset_name via the .load API. ds = Dataset.load(dataset_name) The basic idea is that we don't want to share data directly, instead, we share the recipes for how to re-create Datasets. These recipes are stored in the dataset catalog. Datasets can then be shared by sharing the catalog. Datasets are the fundamental object that makes sharing of datasets reproducible, as they keep track of their own recipes, check that the data created from a recipe has the correct hashes, and keep licenses and other metadata with the data itself.","title":"Datasets"},{"location":"datasets/#what-is-a-dataset-object","text":"A Dataset is the fundamental object we use for turning raw data into useful datasets, reproducibly. It is like a scikit-learn-style Bunch object---essentially, a dictionary with some extra magic to make it nicer to work with---containing the following attributes: data: the processed data target: (optional) target vector (for supervised learning problems) metadata: Data about the data The data attribute can really be any processed data form that you like: sometimes it's a pandas dataframe, a list of tuples containing other data, or other formats including scipy.sparse matrices or igraph graphs. The target (if you're using it), expects something that matches the data in terms of length. For a hint as to which data format to expect, you can look at the contents of the DESCR attribute, one of the many pieces of medata that are maintained as part of the Dataset object. This metadata is where things get interesting... which we'll cover on its own next.","title":"What is a Dataset object?"},{"location":"datasets/#why-metadata","text":"The metadata is where the magic lives. It serves several purposes in terms of bookkeeping: it includes HASHES , which improve data reproducibility , since what you download and process gets checked each step along the way to ensure the raw data matches what is stored in the dataset_catalog , it provides easy access to what the data is via the DESCR attribute, it provides easy (and continual) access to the license / usage restrictions for the data (the LICENSE attribute), which helps with knowing what you can do when Sharing your Work . it provides the extra data manifest , EXTRA , if your dataset includes around additional raw data (extra) files. In short, it helps you to know what data you're working with, what you can do with it, and whether something has gone wrong. Under the hood, metadata is a dictionary; however metadata can also be accessed by referring to attributes expressed in uppercase. For example, ds.metadata['license'] and ds.LICENSE refer to the same thing.","title":"Why metadata?"},{"location":"datasets/#using-a-dataset","text":"As mentioned before, to load a Dataset : ds = Dataset.load(\"<dataset-name>\") At this point, if you already have a cached copy of the desired Dataset on disk, it will load it. Otherwise, the it will follow the recipe for generating the requested Dataset ; i.e. generate the dataset from raw data, as per the instructions contained in the dataset_catalog (described below). Because of licenses and other distribution restrictions, some of the datasets may require a manual download step. If so, you will be prompted at this point and given instructions for what to do. Some datasets will require local pre-processing. If so, the first time your run the command, you will be executing all of the processing scripts (which can be quite slow). After the first load, however, datasets will load from cache on disk which should be fast. If you need to free up space, you can even delete related source files from data/raw and data/interim . Just don't touch the data/processed directory. To access the data, target or metdata: ds.data ds.target ds.metadata To access the most common metadata fields: ds.DESCR # or ds.metadata['descr'] ds.LICENSE # or ds.metadata['license'] ds.HASHES # or ds.metadata['hashes']","title":"Using a Dataset"},{"location":"datasets/#the-dataset-catalog","text":"You can explore all of the currently available Datasets via the dataset catalog. The catalog keeps a record of the recipes used to generate a Dataset along with relevant hashes that are used to ensure the integrity of data when it's loaded. To access the catalog: workflow.available_datasets(keys_only=True) If you're interested, set keys_only=False to see the complete contents of the metadata that is saved in the catalog.","title":"The Dataset catalog"},{"location":"opinions/","text":"Easydata Opinions There are some opinions implicit in the project structure that have grown out of our experience with what works and what doesn't when collaborating on data science projects. Some of the opinions are about workflows, and some of the opinions are about tools that make life easier. Here are some of the beliefs which this project is built on\u2014if you've got thoughts, please contribute or share them . Data is immutable Don't ever edit your raw data, especially not manually. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see Analysis is a DAG ), but anyone should be able to reproduce the final products with only the code in {{ cookiecutter.module_name }} and the data in data/raw . Also, if data is immutable, it doesn't need source control in the same way that code does. Therefore, by default, the data folder is included in the .gitignore file. If you have a small amount of data that rarely changes, you may want to include the data in the repository. Github currently warns if files are over 50MB and rejects files over 100MB. Some other options for storing/syncing large data include AWS S3 with a syncing tool (e.g., s3cmd ), Git Large File Storage , Git Annex , and dat . Shared workflows matter Shared workflows matter in enabling reproducible results and smoother collaboration. That's why we include a suite of recommendeded (but lightweight) workflows that help you to collaborate with others in our framework-docs . Use them out-of-the-box for a workshop, or adapt them to suit your team's needs. Either way, we recommend that shared workflows stay with the project and include a few key elements: Contributor guidelines A shared git workflow How to submit issues, questions, or get help Where to put different types of project materials such as code, notebooks for story-telling, documentation, visualizations, other deliverables Which licenses to use (aka. terms for sharing) Notebooks are for exploration and communication Notebook packages like the Jupyter notebook , Beaker notebook , Zeppelin , and other literate programming tools are very effective for exploratory data analysis. However, these tools can be less effective for reproducing an analysis. When we use notebooks in our work, we often subdivide the notebooks folder. For example, notebooks/exploratory contains initial explorations, whereas notebooks/reports is more polished work that can be exported as html to the reports directory. Since notebooks are challenging objects for source control (e.g., diffs of the json are often not human-readable and merging is near impossible), we recommended not collaborating directly with others on Jupyter notebooks. There are two steps we recommend for using notebooks effectively: Follow a naming convention that shows the owner and the order the analysis was done in. We use the format <step>-<ghuser>-<description>.ipynb (e.g., 0.3-bull-visualize-distributions.ipynb ). Refactor the good parts. Don't write code to do the same task in multiple notebooks. If it's a data preprocessing task, put it in the pipeline at src/data/make_dataset.py and load data from data/interim . If it's useful utility code, refactor it to src . Now by default we turn the project into a Python package (see the setup.py file). You can import your code and use it in notebooks with a cell like the following: # OPTIONAL: Load the \"autoreload\" extension so that code can change %load_ext autoreload # OPTIONAL: always reload modules so that as you change code in src, it gets loaded %autoreload 2 from src.data import make_dataset Analysis is a DAG Often in an analysis you have long-running steps that preprocess data or train models. If these steps have been run already (and you have stored the output somewhere like the data/interim directory), you don't want to wait to rerun them every time. We prefer make for managing steps that depend on each other, especially the long-running ones. Make is a common tool on Unix-based platforms (and is available for Windows ). Following the make documentation , Makefile conventions , and portability guide will help ensure your Makefiles work effectively across systems. Here are some examples to get started . A number of data folks use make as their tool of choice, including Mike Bostock . There are other tools for managing DAGs that are written in Python instead of a DSL (e.g., Paver , Luigi , Airflow , Snakemake , Ruffus , or Joblib ). Feel free to use these if they are more appropriate for your analysis. Build from the environment up The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together. One effective approach to this is use conda By listing all of your requirements in the repository (we include a environment.yml file) you can easily track the packages needed to recreate the analysis. Here is a good workflow: Run make create_environment when creating a new project Add new requirements to environment.yml , either in the main section (for conda installations), or under the indented - pip: line, if it should be pip installed. Type make update_environment If you have more complex requirements for recreating your environment, consider a virtual machine based approach such as Docker or Vagrant . Both of these tools use text-based formats (Dockerfile and Vagrantfile respectively) you can easily add to source control to describe how to create a virtual machine with the requirements you need. Keep secrets and configuration out of version control You really don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the Twelve Factor App principles on this point. Here's one way to do this: Store your secrets and config variables in a special file Create a .env file in the project root folder. Thanks to the .gitignore , this file should never get committed into the version control repository. Here's an example: # example .env file DATABASE_URL=postgres://username:password@localhost:5432/dbname AWS_ACCESS_KEY=myaccesskey AWS_SECRET_ACCESS_KEY=mysecretkey OTHER_VARIABLE=something Use a package to load these variables automatically. If you look at the stub script in src/data/make_dataset.py , it uses a package called python-dotenv to load up all the entries in this file as environment variables so they are accessible with os.environ.get . Here's an example snippet adapted from the python-dotenv documentation: # src/data/dotenv_example.py import os from dotenv import load_dotenv, find_dotenv # find .env automagically by walking up directories until it's found dotenv_path = find_dotenv() # load up the entries as environment variables load_dotenv(dotenv_path) database_url = os.environ.get(\"DATABASE_URL\") other_variable = os.environ.get(\"OTHER_VARIABLE\") Be conservative in changing the default folder structure To keep this structure broadly applicable for many different kinds of projects, we think the best approach is to be liberal in changing the folders around for your project, but be conservative in changing the default structure for all projects.","title":"Opinions"},{"location":"opinions/#easydata-opinions","text":"There are some opinions implicit in the project structure that have grown out of our experience with what works and what doesn't when collaborating on data science projects. Some of the opinions are about workflows, and some of the opinions are about tools that make life easier. Here are some of the beliefs which this project is built on\u2014if you've got thoughts, please contribute or share them .","title":"Easydata Opinions"},{"location":"opinions/#data-is-immutable","text":"Don't ever edit your raw data, especially not manually. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see Analysis is a DAG ), but anyone should be able to reproduce the final products with only the code in {{ cookiecutter.module_name }} and the data in data/raw . Also, if data is immutable, it doesn't need source control in the same way that code does. Therefore, by default, the data folder is included in the .gitignore file. If you have a small amount of data that rarely changes, you may want to include the data in the repository. Github currently warns if files are over 50MB and rejects files over 100MB. Some other options for storing/syncing large data include AWS S3 with a syncing tool (e.g., s3cmd ), Git Large File Storage , Git Annex , and dat .","title":"Data is immutable"},{"location":"opinions/#shared-workflows-matter","text":"Shared workflows matter in enabling reproducible results and smoother collaboration. That's why we include a suite of recommendeded (but lightweight) workflows that help you to collaborate with others in our framework-docs . Use them out-of-the-box for a workshop, or adapt them to suit your team's needs. Either way, we recommend that shared workflows stay with the project and include a few key elements: Contributor guidelines A shared git workflow How to submit issues, questions, or get help Where to put different types of project materials such as code, notebooks for story-telling, documentation, visualizations, other deliverables Which licenses to use (aka. terms for sharing)","title":"Shared workflows matter"},{"location":"opinions/#notebooks-are-for-exploration-and-communication","text":"Notebook packages like the Jupyter notebook , Beaker notebook , Zeppelin , and other literate programming tools are very effective for exploratory data analysis. However, these tools can be less effective for reproducing an analysis. When we use notebooks in our work, we often subdivide the notebooks folder. For example, notebooks/exploratory contains initial explorations, whereas notebooks/reports is more polished work that can be exported as html to the reports directory. Since notebooks are challenging objects for source control (e.g., diffs of the json are often not human-readable and merging is near impossible), we recommended not collaborating directly with others on Jupyter notebooks. There are two steps we recommend for using notebooks effectively: Follow a naming convention that shows the owner and the order the analysis was done in. We use the format <step>-<ghuser>-<description>.ipynb (e.g., 0.3-bull-visualize-distributions.ipynb ). Refactor the good parts. Don't write code to do the same task in multiple notebooks. If it's a data preprocessing task, put it in the pipeline at src/data/make_dataset.py and load data from data/interim . If it's useful utility code, refactor it to src . Now by default we turn the project into a Python package (see the setup.py file). You can import your code and use it in notebooks with a cell like the following: # OPTIONAL: Load the \"autoreload\" extension so that code can change %load_ext autoreload # OPTIONAL: always reload modules so that as you change code in src, it gets loaded %autoreload 2 from src.data import make_dataset","title":"Notebooks are for exploration and communication"},{"location":"opinions/#analysis-is-a-dag","text":"Often in an analysis you have long-running steps that preprocess data or train models. If these steps have been run already (and you have stored the output somewhere like the data/interim directory), you don't want to wait to rerun them every time. We prefer make for managing steps that depend on each other, especially the long-running ones. Make is a common tool on Unix-based platforms (and is available for Windows ). Following the make documentation , Makefile conventions , and portability guide will help ensure your Makefiles work effectively across systems. Here are some examples to get started . A number of data folks use make as their tool of choice, including Mike Bostock . There are other tools for managing DAGs that are written in Python instead of a DSL (e.g., Paver , Luigi , Airflow , Snakemake , Ruffus , or Joblib ). Feel free to use these if they are more appropriate for your analysis.","title":"Analysis is a DAG"},{"location":"opinions/#build-from-the-environment-up","text":"The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together. One effective approach to this is use conda By listing all of your requirements in the repository (we include a environment.yml file) you can easily track the packages needed to recreate the analysis. Here is a good workflow: Run make create_environment when creating a new project Add new requirements to environment.yml , either in the main section (for conda installations), or under the indented - pip: line, if it should be pip installed. Type make update_environment If you have more complex requirements for recreating your environment, consider a virtual machine based approach such as Docker or Vagrant . Both of these tools use text-based formats (Dockerfile and Vagrantfile respectively) you can easily add to source control to describe how to create a virtual machine with the requirements you need.","title":"Build from the environment up"},{"location":"opinions/#keep-secrets-and-configuration-out-of-version-control","text":"You really don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the Twelve Factor App principles on this point. Here's one way to do this:","title":"Keep secrets and configuration out of version control"},{"location":"opinions/#store-your-secrets-and-config-variables-in-a-special-file","text":"Create a .env file in the project root folder. Thanks to the .gitignore , this file should never get committed into the version control repository. Here's an example: # example .env file DATABASE_URL=postgres://username:password@localhost:5432/dbname AWS_ACCESS_KEY=myaccesskey AWS_SECRET_ACCESS_KEY=mysecretkey OTHER_VARIABLE=something","title":"Store your secrets and config variables in a special file"},{"location":"opinions/#use-a-package-to-load-these-variables-automatically","text":"If you look at the stub script in src/data/make_dataset.py , it uses a package called python-dotenv to load up all the entries in this file as environment variables so they are accessible with os.environ.get . Here's an example snippet adapted from the python-dotenv documentation: # src/data/dotenv_example.py import os from dotenv import load_dotenv, find_dotenv # find .env automagically by walking up directories until it's found dotenv_path = find_dotenv() # load up the entries as environment variables load_dotenv(dotenv_path) database_url = os.environ.get(\"DATABASE_URL\") other_variable = os.environ.get(\"OTHER_VARIABLE\")","title":"Use a package to load these variables automatically."},{"location":"opinions/#be-conservative-in-changing-the-default-folder-structure","text":"To keep this structure broadly applicable for many different kinds of projects, we think the best approach is to be liberal in changing the folders around for your project, but be conservative in changing the default structure for all projects.","title":"Be conservative in changing the default folder structure"}]}